{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "615d337b",
   "metadata": {},
   "source": [
    "# Initial dataset exploration - All datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f6898d",
   "metadata": {},
   "source": [
    "The goal of this notebook is to perform an initial dataset exploration to understand the\n",
    "data better. We obtain statistics about the users and items in the dataset.\n",
    "In addition, we calculate the expected number of occurences as positive items in the sampling set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcb3486",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20b8c11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "try:\n",
    "    os.chdir('code')\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "from world import Config, FakeArgs\n",
    "from dataloader import DataLoader\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4056301c",
   "metadata": {},
   "source": [
    "### Analysis function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a42c74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_dataset(config, dataset):\n",
    "    config.dataset = dataset\n",
    "    # Load the dataset\n",
    "    dataset = DataLoader(config)\n",
    "    # Create the user item sparse matrices for the train and test set\n",
    "    train_user_item_matrix = csr_matrix(\n",
    "        (np.ones(len(dataset.df_train['user_id'])), (dataset.df_train['user_id'], dataset.df_train['item_id'])),\n",
    "        shape=(dataset.n_user, dataset.m_item)\n",
    "    )\n",
    "\n",
    "    test_user_item_matrix = csr_matrix(\n",
    "        (np.ones(len(dataset.df_test['user_id'])), (dataset.df_test['user_id'], dataset.df_test['item_id'])),\n",
    "        shape=(dataset.n_user, dataset.m_item)\n",
    "    )\n",
    "    # Get the number of items by users\n",
    "    train_user = np.array(train_user_item_matrix.sum(axis=1)).squeeze()\n",
    "    # Get the number of users by items\n",
    "    train_item = np.array(train_user_item_matrix.sum(axis=0)).squeeze()\n",
    "\n",
    "    # Get the number of items by users\n",
    "    test_user = np.array(test_user_item_matrix.sum(axis=1)).squeeze()\n",
    "    # Get the number of users by items\n",
    "    test_item = np.array(test_user_item_matrix.sum(axis=0)).squeeze()\n",
    "    \n",
    "    print('In the training dataset:')\n",
    "    print(f'- There are {len(dataset.df_train[\"user_id\"])} edges.')\n",
    "    print(f'- The users have a minimum, mean, median, maximum, std of {get_array_statistics(train_user)} edges.')\n",
    "    print(f'- The items have a minimum, mean, median, maximum, std of {get_array_statistics(train_item)} users.\\n')\n",
    "\n",
    "    print('In the testing dataset:')\n",
    "    print(f'- There are {len(dataset.df_test[\"user_id\"])} edges.')\n",
    "    print(f'- The users have a minimum, mean, median, maximum, std of {get_array_statistics(test_user)} edges.')\n",
    "    print(f'- The items have a minimum, mean, median, maximum, std of {get_array_statistics(test_item)} users.\\n')\n",
    "    \n",
    "    excepted_n_item = {i: 0 for i in dataset.df_train['item_id'].unique()}\n",
    "    sample_list = []\n",
    "    for user_id in dataset.df_train['user_id'].unique():\n",
    "        user_items = dataset.all_pos[user_id]\n",
    "        user_items_len = len(user_items)\n",
    "        for i in user_items:\n",
    "            excepted_n_item[i] += dataset.mean_item_per_user / user_items_len\n",
    "    \n",
    "    min_expected_value = min([v for v in excepted_n_item.values()])\n",
    "    mean_expected_value = np.mean([v for v in excepted_n_item.values()])\n",
    "    max_expected_value = max([v for v in excepted_n_item.values()])\n",
    "    std_expected_value = np.std([v for v in excepted_n_item.values()])\n",
    "\n",
    "    print(f'Minimum item expected value: {min_expected_value}')\n",
    "    print(f'Mean item expected value: {mean_expected_value}')\n",
    "    print(f'Maximum item expected value: {max_expected_value}')\n",
    "    print(f'Std item expected value: {std_expected_value}')\n",
    "    \n",
    "    data_dic = {\n",
    "        'dataset': dataset,\n",
    "        'min_expected_value': min_expected_value,\n",
    "        'mean_expected_value': mean_expected_value,\n",
    "        'max_expected_value': max_expected_value,\n",
    "        'std_expected_value': std_expected_value,\n",
    "    }\n",
    "    return data_dic\n",
    "    \n",
    "\n",
    "def get_array_statistics(array):\n",
    "    return array.min(), array.mean(), np.median(array), array.max(), array.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3e1236",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fc4adc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the default values to instantiate the Config class\n",
    "args = FakeArgs()\n",
    "\n",
    "# Instantiate the config classss\n",
    "config = Config(\n",
    "    args.dataset, args.model, args.bpr_batch, args.recdim, args.layer, args.dropout, args.keepprob, args.a_fold,\n",
    "    args.testbatch, args.multicore, args.lr, args.decay, args.pretrain, args.seed, args.epochs, args.load,\n",
    "    args.checkpoint_path, args.results_path, args.topks, args.tensorboard, args.comment, args.sampling\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dc00c4",
   "metadata": {},
   "source": [
    "### Gowalla dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d64885d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;30;43mloading [../data/gowalla]\u001b[0m\n",
      "0 training samples and 0 test samples were dropped during the data cleaning.\n",
      "The user ids were not updated.\n",
      "The item ids were not updated.\n",
      "810128 interactions for training\n",
      "217242 interactions for testing\n",
      "gowalla Sparsity : 0.0008396216228570436\n",
      "gowalla is ready to go\n",
      "In the training dataset:\n",
      "- There are 810128 edges.\n",
      "- The users have a minimum, mean, median, maximum, std of (8.0, 27.132694755174494, 16.0, 811.0, 36.85818812689325) edges.\n",
      "- The items have a minimum, mean, median, maximum, std of (1.0, 19.76838046899783, 12.0, 1415.0, 33.11268050158492) users.\n",
      "\n",
      "In the testing dataset:\n",
      "- There are 217242 edges.\n",
      "- The users have a minimum, mean, median, maximum, std of (1.0, 7.275838971130016, 4.0, 203.0, 9.217030630034714) edges.\n",
      "- The items have a minimum, mean, median, maximum, std of (0.0, 5.301041946267783, 3.0, 895.0, 13.345033564190903) users.\n",
      "\n",
      "Minimum item expected value: 0.14594594594594595\n",
      "Mean item expected value: 19.671701520216686\n",
      "Maximum item expected value: 1936.5016793347356\n",
      "Std item expected value: 37.22464388828108\n"
     ]
    }
   ],
   "source": [
    "gowalla_analysis = analyse_dataset(config, 'gowalla')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15438df2",
   "metadata": {},
   "source": [
    "### Amazon Book dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50fe60ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;30;43mloading [../data/amazon-book]\u001b[0m\n",
      "0 training samples and 0 test samples were dropped during the data cleaning.\n",
      "The user ids were not updated.\n",
      "The item ids were not updated.\n",
      "2380730 interactions for training\n",
      "603378 interactions for testing\n",
      "amazon-book Sparsity : 0.0006188468344849981\n",
      "amazon-book is ready to go\n",
      "In the training dataset:\n",
      "- There are 2380730 edges.\n",
      "- The users have a minimum, mean, median, maximum, std of (16.0, 45.22405637976559, 26.0, 10682.0, 77.95751270232581) edges.\n",
      "- The items have a minimum, mean, median, maximum, std of (1.0, 25.990785925610542, 15.0, 1741.0, 38.39710866361827) users.\n",
      "\n",
      "In the testing dataset:\n",
      "- There are 603378 edges.\n",
      "- The users have a minimum, mean, median, maximum, std of (0.0, 11.461694812225748, 7.0, 2631.0, 18.933403968453813) edges.\n",
      "- The items have a minimum, mean, median, maximum, std of (0.0, 6.587167982183211, 3.0, 416.0, 11.468086187789028) users.\n",
      "\n",
      "Minimum item expected value: 0.004212694252012731\n",
      "Mean item expected value: 25.86201814430288\n",
      "Maximum item expected value: 2170.7665967042512\n",
      "Std item expected value: 43.900911125945775\n"
     ]
    }
   ],
   "source": [
    "gowalla_analysis = analyse_dataset(config, 'amazon-book')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b4e7ee",
   "metadata": {},
   "source": [
    "### Yelp 2018 dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceeacfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;30;43mloading [../data/yelp2018]\u001b[0m\n",
      "0 training samples and 0 test samples were dropped during the data cleaning.\n",
      "The user ids were not updated.\n",
      "The item ids were not updated.\n",
      "1237259 interactions for training\n",
      "324147 interactions for testing\n",
      "yelp2018 Sparsity : 0.0012958757851778647\n",
      "yelp2018 is ready to go\n",
      "In the training dataset:\n",
      "- There are 1237259 edges.\n",
      "- The users have a minimum, mean, median, maximum, std of (16.0, 39.06969180245042, 25.0, 1848.0, 45.10830201741167) edges.\n",
      "- The items have a minimum, mean, median, maximum, std of (1.0, 32.51837153069807, 17.0, 1258.0, 49.266030880676894) users.\n",
      "\n",
      "In the testing dataset:\n",
      "- There are 324147 edges.\n",
      "- The users have a minimum, mean, median, maximum, std of (2.0, 10.235790071996968, 7.0, 463.0, 11.245355174258071) edges.\n",
      "- The items have a minimum, mean, median, maximum, std of (0.0, 8.51942283431455, 5.0, 275.0, 12.541067574469807) users.\n",
      "\n",
      "Minimum item expected value: 0.16956521739130434\n",
      "Mean item expected value: 32.46036585365854\n",
      "Maximum item expected value: 1577.5946938047987\n",
      "Std item expected value: 53.434092876499115\n"
     ]
    }
   ],
   "source": [
    "gowalla_analysis = analyse_dataset(config, 'yelp2018')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e319cac9",
   "metadata": {},
   "source": [
    "### LastFM dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a326d776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;30;43mloading [../data/lastfm]\u001b[0m\n",
      "0 training samples and 0 test samples were dropped during the data cleaning.\n",
      "The user ids were not updated.\n",
      "The item ids were not updated.\n",
      "2418427 interactions for training\n",
      "616336 interactions for testing\n",
      "lastfm Sparsity : 0.0026760006439057356\n",
      "lastfm is ready to go\n",
      "In the training dataset:\n",
      "- There are 2418427 edges.\n",
      "- The users have a minimum, mean, median, maximum, std of (8.0, 102.62356785199016, 42.0, 8384.0, 208.9881824698514) edges.\n",
      "- The items have a minimum, mean, median, maximum, std of (1.0, 50.25511709577541, 24.0, 2836.0, 86.9692285597461) users.\n",
      "\n",
      "In the testing dataset:\n",
      "- There are 616336 edges.\n",
      "- The users have a minimum, mean, median, maximum, std of (2.0, 26.153611134685566, 11.0, 2096.0, 52.24164370985187) edges.\n",
      "- The items have a minimum, mean, median, maximum, std of (0.0, 12.80751407850716, 6.0, 1362.0, 23.542949552649013) users.\n",
      "\n",
      "Minimum item expected value: 0.09705042816365367\n",
      "Mean item expected value: 49.94975375600024\n",
      "Maximum item expected value: 4749.4859887848625\n",
      "Std item expected value: 96.65678354198401\n"
     ]
    }
   ],
   "source": [
    "gowalla_analysis = analyse_dataset(config, 'lastfm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7412e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
