{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20b8c11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "try:\n",
    "    os.chdir('code')\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "from world import Config, FakeArgs\n",
    "from dataloader import DataLoader\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a42c74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_dataset(config, dataset):\n",
    "    config.dataset = dataset\n",
    "    # Load the dataset\n",
    "    dataset = DataLoader(config)\n",
    "    # Create the user item sparse matrices for the train and test set\n",
    "    train_user_item_matrix = csr_matrix(\n",
    "        (np.ones(len(dataset.df_train['user_id'])), (dataset.df_train['user_id'], dataset.df_train['item_id'])),\n",
    "        shape=(dataset.n_user, dataset.m_item)\n",
    "    )\n",
    "\n",
    "    test_user_item_matrix = csr_matrix(\n",
    "        (np.ones(len(dataset.df_test['user_id'])), (dataset.df_test['user_id'], dataset.df_test['item_id'])),\n",
    "        shape=(dataset.n_user, dataset.m_item)\n",
    "    )\n",
    "    # Get the number of items by users\n",
    "    train_user = np.array(train_user_item_matrix.sum(axis=1)).squeeze()\n",
    "    # Get the number of users by items\n",
    "    train_item = np.array(train_user_item_matrix.sum(axis=0)).squeeze()\n",
    "\n",
    "    # Get the number of items by users\n",
    "    test_user = np.array(test_user_item_matrix.sum(axis=1)).squeeze()\n",
    "    # Get the number of users by items\n",
    "    test_item = np.array(test_user_item_matrix.sum(axis=0)).squeeze()\n",
    "    \n",
    "    print('In the training dataset:')\n",
    "    print(f'- There are {len(dataset.df_train[\"user_id\"])} edges.')\n",
    "    print(f'- The users have a minimum, mean, median, maximum, std of {get_array_statistics(train_user)} edges.')\n",
    "    print(f'- The items have a minimum, mean, median, maximum, std of {get_array_statistics(train_item)} users.\\n')\n",
    "\n",
    "    print('In the testing dataset:')\n",
    "    print(f'- There are {len(dataset.df_test[\"user_id\"])} edges.')\n",
    "    print(f'- The users have a minimum, mean, median, maximum, std of {get_array_statistics(test_user)} edges.')\n",
    "    print(f'- The items have a minimum, mean, median, maximum, std of {get_array_statistics(test_item)} users.\\n')\n",
    "    \n",
    "    excepted_n_item = {i: 0 for i in dataset.df_train['item_id'].unique()}\n",
    "    sample_list = []\n",
    "    for user_id in dataset.df_train['user_id'].unique():\n",
    "        user_items = dataset.all_pos[user_id]\n",
    "        user_items_len = len(user_items)\n",
    "        for i in user_items:\n",
    "            excepted_n_item[i] += dataset.mean_item_per_user / user_items_len\n",
    "    \n",
    "    min_expected_value = min([v for v in excepted_n_item.values()])\n",
    "    mean_expected_value = np.mean([v for v in excepted_n_item.values()])\n",
    "    max_expected_value = max([v for v in excepted_n_item.values()])\n",
    "    std_expected_value = np.std([v for v in excepted_n_item.values()])\n",
    "\n",
    "    print(f'Minimum item expected value: {min_expected_value}')\n",
    "    print(f'Mean item expected value: {mean_expected_value}')\n",
    "    print(f'Maximum item expected value: {max_expected_value}')\n",
    "    print(f'Std item expected value: {std_expected_value}')\n",
    "    \n",
    "    data_dic = {\n",
    "        'dataset': dataset,\n",
    "        'min_expected_value': min_expected_value,\n",
    "        'mean_expected_value': mean_expected_value,\n",
    "        'max_expected_value': max_expected_value,\n",
    "        'std_expected_value': std_expected_value,\n",
    "    }\n",
    "    return data_dic\n",
    "    \n",
    "\n",
    "def get_array_statistics(array):\n",
    "    return array.min(), array.mean(), np.median(array), array.max(), array.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fc4adc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;30;43mloading [../data/gowalla]\u001b[0m\n",
      "0 training samples and 0 test samples were dropped during the data cleaning.\n",
      "The user ids were not updated.\n",
      "The item ids were not updated.\n",
      "810128 interactions for training\n",
      "217242 interactions for testing\n",
      "gowalla Sparsity : 0.0008396216228570436\n",
      "gowalla is ready to go\n",
      "In the training dataset:\n",
      "- There are 810128 edges.\n",
      "- The users have a minimum, mean, median, maximum, std of (8.0, 27.132694755174494, 16.0, 811.0, 36.85818812689325) edges.\n",
      "- The items have a minimum, mean, median, maximum, std of (1.0, 19.76838046899783, 12.0, 1415.0, 33.11268050158492) users.\n",
      "\n",
      "In the testing dataset:\n",
      "- There are 217242 edges.\n",
      "- The users have a minimum, mean, median, maximum, std of (1.0, 7.275838971130016, 4.0, 203.0, 9.217030630034714) edges.\n",
      "- The items have a minimum, mean, median, maximum, std of (0.0, 5.301041946267783, 3.0, 895.0, 13.345033564190903) users.\n",
      "\n",
      "Minimum item expected value: 0.14594594594594595\n",
      "Mean item expected value: 19.671701520216686\n",
      "Maximum item expected value: 1936.5016793347356\n",
      "Std item expected value: 37.22464388828108\n",
      "\u001b[0;30;43mloading [../data/yelp2018]\u001b[0m\n",
      "0 training samples and 0 test samples were dropped during the data cleaning.\n",
      "The user ids were not updated.\n",
      "The item ids were not updated.\n",
      "1237259 interactions for training\n",
      "324147 interactions for testing\n",
      "yelp2018 Sparsity : 0.0012958757851778647\n",
      "yelp2018 is ready to go\n",
      "In the training dataset:\n",
      "- There are 1237259 edges.\n",
      "- The users have a minimum, mean, median, maximum, std of (16.0, 39.06969180245042, 25.0, 1848.0, 45.10830201741167) edges.\n",
      "- The items have a minimum, mean, median, maximum, std of (1.0, 32.51837153069807, 17.0, 1258.0, 49.266030880676894) users.\n",
      "\n",
      "In the testing dataset:\n",
      "- There are 324147 edges.\n",
      "- The users have a minimum, mean, median, maximum, std of (2.0, 10.235790071996968, 7.0, 463.0, 11.245355174258071) edges.\n",
      "- The items have a minimum, mean, median, maximum, std of (0.0, 8.51942283431455, 5.0, 275.0, 12.541067574469807) users.\n",
      "\n",
      "Minimum item expected value: 0.16956521739130434\n",
      "Mean item expected value: 32.46036585365854\n",
      "Maximum item expected value: 1577.5946938047987\n",
      "Std item expected value: 53.434092876499115\n",
      "\u001b[0;30;43mloading [../data/amazon-book]\u001b[0m\n",
      "0 training samples and 0 test samples were dropped during the data cleaning.\n",
      "The user ids were not updated.\n",
      "The item ids were not updated.\n",
      "2380730 interactions for training\n",
      "603378 interactions for testing\n",
      "amazon-book Sparsity : 0.0006188468344849981\n",
      "amazon-book is ready to go\n",
      "In the training dataset:\n",
      "- There are 2380730 edges.\n",
      "- The users have a minimum, mean, median, maximum, std of (16.0, 45.22405637976559, 26.0, 10682.0, 77.95751270232581) edges.\n",
      "- The items have a minimum, mean, median, maximum, std of (1.0, 25.990785925610542, 15.0, 1741.0, 38.39710866361827) users.\n",
      "\n",
      "In the testing dataset:\n",
      "- There are 603378 edges.\n",
      "- The users have a minimum, mean, median, maximum, std of (0.0, 11.461694812225748, 7.0, 2631.0, 18.933403968453813) edges.\n",
      "- The items have a minimum, mean, median, maximum, std of (0.0, 6.587167982183211, 3.0, 416.0, 11.468086187789028) users.\n",
      "\n",
      "Minimum item expected value: 0.004212694252012731\n",
      "Mean item expected value: 25.86201814430288\n",
      "Maximum item expected value: 2170.7665967042512\n",
      "Std item expected value: 43.900911125945775\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgowalla\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myelp2018\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamazon-book\u001b[39m\u001b[38;5;124m'\u001b[39m]: \u001b[38;5;66;03m# 'lastfm', \u001b[39;00m\n\u001b[1;32m     12\u001b[0m     data_list\u001b[38;5;241m.\u001b[39mappend(analyse_dataset(config, dataset))\n\u001b[0;32m---> 14\u001b[0m df_analysis \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(data_list)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract the default values to instantiate the Config class\n",
    "args = FakeArgs()\n",
    "\n",
    "# Instantiate the config classss\n",
    "config = Config(\n",
    "    args.dataset, args.model, args.bpr_batch, args.recdim, args.layer, args.dropout, args.keepprob, args.a_fold,\n",
    "    args.testbatch, args.multicore, args.lr, args.decay, args.pretrain, args.seed, args.epochs, args.load,\n",
    "    args.checkpoint_path, args.results_path, args.topks, args.tensorboard, args.comment, args.sampling\n",
    ")\n",
    "data_list = []\n",
    "for dataset in ['gowalla', 'yelp2018', 'amazon-book']: # 'lastfm', \n",
    "    data_list.append(analyse_dataset(config, dataset))\n",
    "    \n",
    "df_analysis = pd.DataFrame(data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fc2e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the user item sparse matrices for the train and test set\n",
    "train_user_item_matrix = csr_matrix(\n",
    "    (np.ones(len(dataset.df_train['user_id'])), (dataset.df_train['user_id'], dataset.df_train['item_id'])),\n",
    "    shape=(dataset.n_user, dataset.m_item)\n",
    ")\n",
    "\n",
    "test_user_item_matrix = csr_matrix(\n",
    "    (np.ones(len(dataset.df_test['user_id'])), (dataset.df_test['user_id'], dataset.df_test['item_id'])),\n",
    "    shape=(dataset.n_user, dataset.m_item)\n",
    ")\n",
    "# Get the number of items by users\n",
    "train_user = np.array(train_user_item_matrix.sum(axis=1)).squeeze()\n",
    "# Get the number of users by items\n",
    "train_item = np.array(train_user_item_matrix.sum(axis=0)).squeeze()\n",
    "\n",
    "# Get the number of items by users\n",
    "test_user = np.array(test_user_item_matrix.sum(axis=1)).squeeze()\n",
    "# Get the number of users by items\n",
    "test_item = np.array(test_user_item_matrix.sum(axis=0)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76d4c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_array_statistics(array):\n",
    "    return array.min(), array.mean(), np.median(array), array.max(), array.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c9c841",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24230bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "excepted_n_item = {i: 0 for i in dataset.df_train['item_id'].unique()}\n",
    "sample_list = []\n",
    "for user_id in dataset.df_train['user_id'].unique():\n",
    "    user_items = dataset.all_pos[user_id]\n",
    "    user_items_len = len(user_items)\n",
    "    for i in user_items:\n",
    "        excepted_n_item[i] += dataset.mean_item_per_user / user_items_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f8db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_expected_value = min([v for v in excepted_n_item.values()])\n",
    "mean_expected_value = np.mean([v for v in excepted_n_item.values()])\n",
    "max_expected_value = max([v for v in excepted_n_item.values()])\n",
    "std_expected_value = np.std([v for v in excepted_n_item.values()])\n",
    "\n",
    "print(f'Minimum item expected value: {min_expected_value}')\n",
    "print(f'Mean item expected value: {mean_expected_value}')\n",
    "print(f'Maximum item expected value: {max_expected_value}')\n",
    "print(f'Std item expected value: {std_expected_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7412e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
