{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f17627a2",
   "metadata": {},
   "source": [
    "## Initial data exploration - Amazon Book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1086cbbd",
   "metadata": {},
   "source": [
    "This notebook simply calculates some statistics to understand the dataset better.\n",
    "\n",
    "We look at how many items and users are in the train and test sets. We also calculate the expected number of appearance of items as positive in the sampling set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335a7f47",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20b8c11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "try:\n",
    "    os.chdir('code')\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "from world import Config, FakeArgs\n",
    "from dataloader import DataLoader\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2397e94b",
   "metadata": {},
   "source": [
    "### Define analysis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee514285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_dataset(config, dataset):\n",
    "    config.dataset = dataset\n",
    "    # Load the dataset\n",
    "    dataset = DataLoader(config)\n",
    "    # Create the user item sparse matrices for the train and test set\n",
    "    train_user_item_matrix = csr_matrix(\n",
    "        (np.ones(len(dataset.df_train['user_id'])), (dataset.df_train['user_id'], dataset.df_train['item_id'])),\n",
    "        shape=(dataset.n_user, dataset.m_item)\n",
    "    )\n",
    "\n",
    "    test_user_item_matrix = csr_matrix(\n",
    "        (np.ones(len(dataset.df_test['user_id'])), (dataset.df_test['user_id'], dataset.df_test['item_id'])),\n",
    "        shape=(dataset.n_user, dataset.m_item)\n",
    "    )\n",
    "    # Get the number of items by users\n",
    "    train_user = np.array(train_user_item_matrix.sum(axis=1)).squeeze()\n",
    "    # Get the number of users by items\n",
    "    train_item = np.array(train_user_item_matrix.sum(axis=0)).squeeze()\n",
    "\n",
    "    # Get the number of items by users\n",
    "    test_user = np.array(test_user_item_matrix.sum(axis=1)).squeeze()\n",
    "    # Get the number of users by items\n",
    "    test_item = np.array(test_user_item_matrix.sum(axis=0)).squeeze()\n",
    "    \n",
    "    # Print the dataset statistics\n",
    "    print('In the training dataset:')\n",
    "    print(f'- There are {len(dataset.df_train[\"user_id\"])} edges.')\n",
    "    print(f'- The users have a minimum, mean, median, maximum, std of {get_array_statistics(train_user)} edges.')\n",
    "    print(f'- The items have a minimum, mean, median, maximum, std of {get_array_statistics(train_item)} users.\\n')\n",
    "\n",
    "    print('In the testing dataset:')\n",
    "    print(f'- There are {len(dataset.df_test[\"user_id\"])} edges.')\n",
    "    print(f'- The users have a minimum, mean, median, maximum, std of {get_array_statistics(test_user)} edges.')\n",
    "    print(f'- The items have a minimum, mean, median, maximum, std of {get_array_statistics(test_item)} users.\\n')\n",
    "    \n",
    "    # Expected number of occurrences of items as positives under random sampling\n",
    "    excepted_n_item = {i: 0 for i in dataset.df_train['item_id'].unique()}\n",
    "    sample_list = []\n",
    "    for user_id in dataset.df_train['user_id'].unique():\n",
    "        user_items = dataset.all_pos[user_id]\n",
    "        user_items_len = len(user_items)\n",
    "        for i in user_items:\n",
    "            excepted_n_item[i] += dataset.mean_item_per_user / user_items_len\n",
    "    \n",
    "    # Print number of occurrences statistics\n",
    "    min_expected_value = min([v for v in excepted_n_item.values()])\n",
    "    mean_expected_value = np.mean([v for v in excepted_n_item.values()])\n",
    "    max_expected_value = max([v for v in excepted_n_item.values()])\n",
    "    std_expected_value = np.std([v for v in excepted_n_item.values()])\n",
    "\n",
    "    print(f'Minimum item expected value: {min_expected_value}')\n",
    "    print(f'Mean item expected value: {mean_expected_value}')\n",
    "    print(f'Maximum item expected value: {max_expected_value}')\n",
    "    print(f'Std item expected value: {std_expected_value}')\n",
    "    \n",
    "    data_dic = {\n",
    "        'dataset': dataset,\n",
    "        'min_expected_value': min_expected_value,\n",
    "        'mean_expected_value': mean_expected_value,\n",
    "        'max_expected_value': max_expected_value,\n",
    "        'std_expected_value': std_expected_value,\n",
    "    }\n",
    "    return data_dic\n",
    "    \n",
    "\n",
    "def get_array_statistics(array):\n",
    "    return array.min(), array.mean(), np.median(array), array.max(), array.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dcd4c9",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3078bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the default values to instantiate the Config class\n",
    "args = FakeArgs()\n",
    "\n",
    "# Instantiate the config classss\n",
    "config = Config(\n",
    "    args.dataset, args.model, args.bpr_batch, args.recdim, args.layer, args.dropout, args.keepprob, args.a_fold,\n",
    "    args.testbatch, args.multicore, args.lr, args.decay, args.pretrain, args.seed, args.epochs, args.load,\n",
    "    args.checkpoint_path, args.results_path, args.topks, args.tensorboard, args.comment, args.sampling\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69c7374",
   "metadata": {},
   "source": [
    "### Gowalla dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abba94e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[0;30;43mloading [../data/gowalla]\u001B[0m\n",
      "0 training samples and 0 test samples were dropped during the data cleaning.\n",
      "The user ids were not updated.\n",
      "The item ids were not updated.\n",
      "810128 interactions for training\n",
      "217242 interactions for testing\n",
      "gowalla Sparsity : 0.0008396216228570436\n",
      "gowalla is ready to go\n",
      "In the training dataset:\n",
      "- There are 810128 edges.\n",
      "- The users have a minimum, mean, median, maximum, std of (8.0, 27.132694755174494, 16.0, 811.0, 36.85818812689325) edges.\n",
      "- The items have a minimum, mean, median, maximum, std of (1.0, 19.76838046899783, 12.0, 1415.0, 33.11268050158492) users.\n",
      "\n",
      "In the testing dataset:\n",
      "- There are 217242 edges.\n",
      "- The users have a minimum, mean, median, maximum, std of (1.0, 7.275838971130016, 4.0, 203.0, 9.217030630034714) edges.\n",
      "- The items have a minimum, mean, median, maximum, std of (0.0, 5.301041946267783, 3.0, 895.0, 13.345033564190903) users.\n",
      "\n",
      "Minimum item expected value: 0.14594594594594595\n",
      "Mean item expected value: 19.671701520216686\n",
      "Maximum item expected value: 1936.5016793347356\n",
      "Std item expected value: 37.22464388828108\n"
     ]
    }
   ],
   "source": [
    "gowalla_analysis = analyse_dataset(config, 'gowalla')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2bd26f",
   "metadata": {},
   "source": [
    "### Yelp 2018 dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cb1a6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[0;30;43mloading [../data/yelp2018]\u001B[0m\n",
      "0 training samples and 0 test samples were dropped during the data cleaning.\n",
      "The user ids were not updated.\n",
      "The item ids were not updated.\n",
      "1237259 interactions for training\n",
      "324147 interactions for testing\n",
      "yelp2018 Sparsity : 0.0012958757851778647\n",
      "yelp2018 is ready to go\n",
      "In the training dataset:\n",
      "- There are 1237259 edges.\n",
      "- The users have a minimum, mean, median, maximum, std of (16.0, 39.06969180245042, 25.0, 1848.0, 45.10830201741167) edges.\n",
      "- The items have a minimum, mean, median, maximum, std of (1.0, 32.51837153069807, 17.0, 1258.0, 49.266030880676894) users.\n",
      "\n",
      "In the testing dataset:\n",
      "- There are 324147 edges.\n",
      "- The users have a minimum, mean, median, maximum, std of (2.0, 10.235790071996968, 7.0, 463.0, 11.245355174258071) edges.\n",
      "- The items have a minimum, mean, median, maximum, std of (0.0, 8.51942283431455, 5.0, 275.0, 12.541067574469807) users.\n",
      "\n",
      "Minimum item expected value: 0.16956521739130434\n",
      "Mean item expected value: 32.46036585365854\n",
      "Maximum item expected value: 1577.5946938047987\n",
      "Std item expected value: 53.434092876499115\n"
     ]
    }
   ],
   "source": [
    "yelp_analysis = analyse_dataset(config, 'yelp2018')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f4ed08",
   "metadata": {},
   "source": [
    "### Amazon Book analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da8ee251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[0;30;43mloading [../data/amazon-book]\u001B[0m\n",
      "0 training samples and 0 test samples were dropped during the data cleaning.\n",
      "The user ids were not updated.\n",
      "The item ids were not updated.\n",
      "2380730 interactions for training\n",
      "603378 interactions for testing\n",
      "amazon-book Sparsity : 0.0006188468344849981\n",
      "amazon-book is ready to go\n",
      "In the training dataset:\n",
      "- There are 2380730 edges.\n",
      "- The users have a minimum, mean, median, maximum, std of (16.0, 45.22405637976559, 26.0, 10682.0, 77.95751270232581) edges.\n",
      "- The items have a minimum, mean, median, maximum, std of (1.0, 25.990785925610542, 15.0, 1741.0, 38.39710866361827) users.\n",
      "\n",
      "In the testing dataset:\n",
      "- There are 603378 edges.\n",
      "- The users have a minimum, mean, median, maximum, std of (0.0, 11.461694812225748, 7.0, 2631.0, 18.933403968453813) edges.\n",
      "- The items have a minimum, mean, median, maximum, std of (0.0, 6.587167982183211, 3.0, 416.0, 11.468086187789028) users.\n",
      "\n",
      "Minimum item expected value: 0.004212694252012731\n",
      "Mean item expected value: 25.86201814430288\n",
      "Maximum item expected value: 2170.7665967042512\n",
      "Std item expected value: 43.900911125945775\n"
     ]
    }
   ],
   "source": [
    "amazon_book_analysis = analyse_dataset(config, 'amazon-book')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075320fd",
   "metadata": {},
   "source": [
    "### LastFM dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feb1d744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[0;30;43mloading [../data/lastfm]\u001B[0m\n",
      "0 training samples and 0 test samples were dropped during the data cleaning.\n",
      "The user ids were not updated.\n",
      "The item ids were not updated.\n",
      "2418427 interactions for training\n",
      "616336 interactions for testing\n",
      "lastfm Sparsity : 0.0026760006439057356\n",
      "lastfm is ready to go\n",
      "In the training dataset:\n",
      "- There are 2418427 edges.\n",
      "- The users have a minimum, mean, median, maximum, std of (8.0, 102.62356785199016, 42.0, 8384.0, 208.9881824698514) edges.\n",
      "- The items have a minimum, mean, median, maximum, std of (1.0, 50.25511709577541, 24.0, 2836.0, 86.9692285597461) users.\n",
      "\n",
      "In the testing dataset:\n",
      "- There are 616336 edges.\n",
      "- The users have a minimum, mean, median, maximum, std of (2.0, 26.153611134685566, 11.0, 2096.0, 52.24164370985187) edges.\n",
      "- The items have a minimum, mean, median, maximum, std of (0.0, 12.80751407850716, 6.0, 1362.0, 23.542949552649013) users.\n",
      "\n",
      "Minimum item expected value: 0.09705042816365367\n",
      "Mean item expected value: 49.94975375600024\n",
      "Maximum item expected value: 4749.4859887848625\n",
      "Std item expected value: 96.65678354198401\n"
     ]
    }
   ],
   "source": [
    "lastfm_analysis = analyse_dataset(config, 'lastfm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed061d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9ae806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
